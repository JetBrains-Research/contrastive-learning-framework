name: transformer

seed: 9
num_workers: 8
log_offline: false

num_classes: 128

# data keys
data_folder: data
train_holdout: train
val_holdout: val
test_holdout: test

save_every_epoch: 1
val_every_epoch: 1
val_check_interval: 0.5
log_every_n_steps: 200
progress_bar_refresh_rate: 1

hyper_parameters:
  n_epochs: 10
  patience: 1
  batch_size: 80
  test_batch_size: 80
  clip_norm: 1
  shuffle_data: true

dataset:
  dir: raw
  name: codeforces
  tokenizer_name: model.yttm
  vocab_size: 40000
  pad_id: 0
  unk_id: 1
  bos_id: 2
  eos_id: 3

encoder:
  hidden_size: 128
  num_classes: 128
  vocab_size: 40000
  num_heads: 8
  num_layers: 4
  dim_feedforward: 1024
  max_seq_len: 386

ssl:
  name: SwAV
  gpus: 1
  batch_size: 80
  num_nodes: 1
  hidden_mlp: 128
  feat_dim: 128
  warmup_epochs: 1
  max_epochs: 10
  nmb_prototypes: 100
  freeze_prototypes_epochs: 1
  temperature: 0.1
  sinkhorn_iterations: 3
  optimizer: "adam"
  exclude_bn_bias: False
  start_lr: 0.
  learning_rate: 1e-3
  final_lr: 0.
  weight_decay: 1e-6
  epsilon: 0.05
  normalize: True